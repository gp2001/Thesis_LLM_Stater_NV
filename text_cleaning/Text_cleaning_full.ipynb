{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "436fc8c3-8980-4f38-9df3-f08367e38136",
      "metadata": {
        "id": "436fc8c3-8980-4f38-9df3-f08367e38136"
      },
      "source": [
        "Preprocessing steps:\n",
        "The preprocessing stage in natural language processing (NLP) involves transforming raw text data into a format that can be more effectively analyzed and used by machine learning models.\n",
        "This stage is crucial for enhancing the quality of the data and ensuring that the models can learn effectively from it. Here's an overview of what happens during the preprocessing stage and why each step is important:\n",
        "\n",
        "Preprocessing: Involves initial steps to clean and prepare raw data for further processing. This includes actions like removing duplicates, filtering out irrelevant or small files, and general data cleaning.\n",
        "\n",
        "Remove:\n",
        "        - Duplicate files\n",
        "        - Non dutch files\n",
        "        - Swear words\n",
        "        - Smaller files\n",
        "\n",
        "Why this order: Duplicate files will remove many files and do not effect the other processing steps. special characters will follow up to find the following items easier. Then non dutch files are removed, this needs to be done before swear words because in the blacklist some words do also excist in english. the last steps are not that important in which order to do so.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03638283-e27e-44dd-81ba-33ed6fbc3fd3",
      "metadata": {
        "id": "03638283-e27e-44dd-81ba-33ed6fbc3fd3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import hashlib\n",
        "import string\n",
        "from langdetect import detect\n",
        "import openpyxl\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce6f35d1-94a3-40d4-99f1-b940b625bfda",
      "metadata": {
        "id": "ce6f35d1-94a3-40d4-99f1-b940b625bfda"
      },
      "outputs": [],
      "source": [
        "folders = [\n",
        "    '../input/data_folder/Part1',\n",
        "    '../input/data_folder/part2',\n",
        "    '../input/data_folder/part3',\n",
        "    '../input/data_folder/part4',\n",
        "    '../input/data_folder/part5',\n",
        "    '../input/data_folder/part6'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2f122e9-9aef-4074-b0ae-3c026804ff24",
      "metadata": {
        "tags": [],
        "id": "a2f122e9-9aef-4074-b0ae-3c026804ff24"
      },
      "outputs": [],
      "source": [
        "# function calculates and returns the MD5 hash of the contents of a specified file\n",
        "def hash_file(filepath):\n",
        "    try:\n",
        "        hasher = hashlib.md5()\n",
        "        with open(filepath, 'rb') as file:\n",
        "            buf = file.read()\n",
        "            hasher.update(buf)\n",
        "        return hasher.hexdigest()\n",
        "    except Exception as e:\n",
        "        print(f\"Could not hash file {filepath}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to find and move duplicate files\n",
        "def Duplicates(folders, DuplicateFolder):\n",
        "    if not os.path.exists(DuplicateFolder):\n",
        "        os.makedirs(DuplicateFolder)\n",
        "    hashes = {}\n",
        "    duplicates = []\n",
        "\n",
        "    try:\n",
        "        for folder in folders:\n",
        "            for root, _, files in os.walk(folder):\n",
        "                for filename in files:\n",
        "                    if filename.endswith('.txt'):\n",
        "                        filepath = os.path.join(root, filename)\n",
        "                        file_hash = hash_file(filepath)\n",
        "                        if file_hash is None:\n",
        "                            continue\n",
        "                        if file_hash in hashes:\n",
        "                            duplicates.append(filepath)\n",
        "                            duplicate_target = os.path.join(DuplicateFolder, os.path.relpath(filepath, folder))\n",
        "                            os.makedirs(os.path.dirname(duplicate_target), exist_ok=True)\n",
        "                            shutil.move(filepath, duplicate_target)\n",
        "                        else:\n",
        "                            hashes[file_hash] = filepath\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred: {e}\")\n",
        "        return\n",
        "\n",
        "    if duplicates:\n",
        "        print(f\"\\nTotal number of duplicate files moved: {len(duplicates)}\")\n",
        "    else:\n",
        "        print(\"No duplicate text files found.\")\n",
        "\n",
        "# Function to move small files\n",
        "def MoveSmallFiles(folders, small_files_folder):\n",
        "    min_size = 3 * 1024  # Minimum size in bytes (3 KB)\n",
        "    if not os.path.exists(small_files_folder):\n",
        "        os.makedirs(small_files_folder)\n",
        "    for folder in folders:\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                try:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    if os.path.getsize(file_path) < min_size:\n",
        "                        relative_path = os.path.relpath(file_path, folder)\n",
        "                        target_path = os.path.join(small_files_folder, relative_path)\n",
        "                        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "                        shutil.move(file_path, target_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to move {file_path}: {e}\")\n",
        "\n",
        "# Function to detect the language of the file's content\n",
        "def detect_language(text):\n",
        "    try:\n",
        "        return detect(text)\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "# Function to move non-Dutch files\n",
        "def DutchFiles(folders, non_dutch_txt):\n",
        "    if not os.path.exists(non_dutch_txt):\n",
        "        os.makedirs(non_dutch_txt)\n",
        "    for folder in folders:\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                try:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                        content = f.read()\n",
        "                    language = detect_language(content)\n",
        "                    if language != 'nl':\n",
        "                        relative_path = os.path.relpath(file_path, folder)\n",
        "                        target_path = os.path.join(non_dutch_txt, relative_path)\n",
        "                        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "                        shutil.move(file_path, target_path)\n",
        "                except Exception as e:\n",
        "                    print(f\"Could not process file {file_path}: {e}\")\n",
        "\n",
        "# Function to load bad words from an Excel file\n",
        "def load_bad_words(BadWords):\n",
        "    bad_words = set()\n",
        "    try:\n",
        "        workbook = openpyxl.load_workbook(BadWords)\n",
        "        sheet = workbook.active\n",
        "        for row in sheet.iter_rows():\n",
        "            for cell in row:\n",
        "                if cell.value:\n",
        "                    bad_words.add(cell.value.strip().lower())\n",
        "        workbook.close()\n",
        "    except Exception as e:\n",
        "        print(\"load_bad_words:\", e)\n",
        "    return bad_words\n",
        "\n",
        "# Function to check if a file contains bad words\n",
        "def file_contains_bad_words(file_path, BadWords):\n",
        "    Max_badword_count = 3\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read().lower()\n",
        "        words = content.split()\n",
        "        bad_word_count = sum(1 for word in words if word in BadWords)\n",
        "        return bad_word_count >= Max_badword_count\n",
        "    except Exception as e:\n",
        "        print(\"file_contains_bad_words:\", e)\n",
        "        return False\n",
        "\n",
        "# Function to move files containing bad words\n",
        "def move_files_with_bad_words(folders, BadWords, destination_folder):\n",
        "    BadWords = load_bad_words(BadWords)\n",
        "    if not BadWords:\n",
        "        print(\"No bad words loaded. Exiting function.\")\n",
        "        return\n",
        "    if not os.path.exists(destination_folder):\n",
        "        os.makedirs(destination_folder)\n",
        "    for folder in folders:\n",
        "        try:\n",
        "            for root, _, files in os.walk(folder):\n",
        "                for file in files:\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    if file_contains_bad_words(file_path, BadWords):\n",
        "                        relative_path = os.path.relpath(file_path, folder)\n",
        "                        target_path = os.path.join(destination_folder, relative_path)\n",
        "                        os.makedirs(os.path.dirname(target_path), exist_ok=True)\n",
        "                        shutil.move(file_path, target_path)\n",
        "        except Exception as e:\n",
        "            print(\"move_files_with_bad_words:\", e)\n",
        "\n",
        "\n",
        "\n",
        "# Duplicate files\n",
        "DuplicateFolder = 'duplicates'\n",
        "Duplicates(folders, DuplicateFolder)\n",
        "\n",
        "# Small files\n",
        "small_files_folder = 'SmallFiles'\n",
        "MoveSmallFiles(folders, small_files_folder)\n",
        "\n",
        "# Non-Dutch files\n",
        "non_dutch_txt = 'NonDutch1'\n",
        "DutchFiles(folders, non_dutch_txt)\n",
        "\n",
        "# Files containing bad words\n",
        "BadWords = 'Badwords.xlsx'\n",
        "destination_folder = 'BadWords_files'\n",
        "move_files_with_bad_words(folders, BadWords, destination_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1201f24b-f753-4c29-b205-41c36f15f7a0",
      "metadata": {
        "id": "1201f24b-f753-4c29-b205-41c36f15f7a0"
      },
      "source": [
        "Normalization\n",
        "Remove emoji and emoticons\n",
        "remove URL and HTML tags\n",
        "Remove accents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0d9b40-3570-4240-b441-cc7e3a6726d4",
      "metadata": {
        "id": "ab0d9b40-3570-4240-b441-cc7e3a6726d4"
      },
      "source": [
        "Remove special characters: Only keep alphanumeric characters, dots, dashes, and underscores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae83a901-75b9-468b-bc09-06439aa679f0",
      "metadata": {
        "id": "ae83a901-75b9-468b-bc09-06439aa679f0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def replace_accented_characters(text):\n",
        "    replacements = {\n",
        "        'ë': 'e', 'ï': 'i', 'é': 'e', 'è': 'e', 'ö': 'o', 'ê': 'e',\n",
        "        'ü': 'u', 'ç': 'c', 'à': 'a', 'û': 'u', 'î': 'i', 'ñ': 'n',\n",
        "        'ä': 'a', 'ô': 'o'\n",
        "    }\n",
        "    for key, value in replacements.items():\n",
        "        text = text.replace(key, value)\n",
        "    return text\n",
        "\n",
        "def process_files(folders, output_folder):\n",
        "    try:\n",
        "        if not os.path.exists(output_folder):\n",
        "            os.makedirs(output_folder)\n",
        "    except OSError as e:\n",
        "        print(f\"Error creating output folder {output_folder}: {e}\")\n",
        "        return\n",
        "\n",
        "    for folder in folders:\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                input_file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "                        text = f.read()\n",
        "                except FileNotFoundError as e:\n",
        "                    print(f\"File not found: {input_file_path}: {e}\")\n",
        "                    continue\n",
        "                except IOError as e:\n",
        "                    print(f\"Error reading file {input_file_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    processed_text = replace_accented_characters(text)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {input_file_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                relative_path = os.path.relpath(root, folder)\n",
        "                output_file_folder = os.path.join(output_folder, relative_path)\n",
        "                try:\n",
        "                    if not os.path.exists(output_file_folder):\n",
        "                        os.makedirs(output_file_folder)\n",
        "                except OSError as e:\n",
        "                    print(f\"Error creating directory {output_file_folder}: {e}\")\n",
        "                    continue\n",
        "\n",
        "                output_file_path = os.path.join(output_file_folder, file)\n",
        "                try:\n",
        "                    with open(output_file_path, 'w', encoding='utf-8') as f:\n",
        "                        f.write(processed_text)\n",
        "                except IOError as e:\n",
        "                    print(f\"Error writing file {output_file_path}: {e}\")\n",
        "                    continue\n",
        "\n",
        "\n",
        "output_folder = 'Processed'\n",
        "\n",
        "process_files(folders, output_folder)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c2f3722-a942-44cb-9844-1106b9c79dfe",
      "metadata": {
        "id": "2c2f3722-a942-44cb-9844-1106b9c79dfe"
      },
      "source": [
        "remove HTML tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec873a42-f4cd-4777-8cf6-3e52e3aefe9e",
      "metadata": {
        "id": "ec873a42-f4cd-4777-8cf6-3e52e3aefe9e"
      },
      "outputs": [],
      "source": [
        "# Function to remove HTML tags\n",
        "def remove_html_tags(text):\n",
        "    html_pattern = re.compile(r'<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "# Function to process files in a folder\n",
        "def process_folder(folder_path):\n",
        "    try:\n",
        "        # Iterate through all files in the folder\n",
        "        for filename in os.listdir(folder_path):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "            # Check if it's a file\n",
        "            if os.path.isfile(file_path):\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                        content = file.read()\n",
        "                    cleaned_content = remove_html_tags(content)\n",
        "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "                        file.write(cleaned_content)\n",
        "                    #print(f\"Processed file: {filename}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {filename}: {e}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing folder {folder_path}: {e}\")\n",
        "\n",
        "\n",
        "folder_path = output_folder\n",
        "process_folder(folder_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3db94383-0976-4ae7-80d9-9525d771bdff",
      "metadata": {
        "id": "3db94383-0976-4ae7-80d9-9525d771bdff"
      },
      "source": [
        "remove emoticons and emojis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5d8c67a-fdf9-47c1-a8c7-50c2cbf356c0",
      "metadata": {
        "id": "a5d8c67a-fdf9-47c1-a8c7-50c2cbf356c0"
      },
      "outputs": [],
      "source": [
        "# Function to remove emojis from text\n",
        "def remove_emoji(text):\n",
        "    try:\n",
        "        emoji_pattern = re.compile(\"[\"\n",
        "                                   u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                                   u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                                   u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                                   u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                                   u\"\\U00002702-\\U000027B0\"\n",
        "                                   u\"\\U000024C2-\\U0001F251\"\n",
        "                                   \"]+\", flags=re.UNICODE)\n",
        "        return emoji_pattern.sub(r'', text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error removing emojis: {e}\")\n",
        "        return text\n",
        "\n",
        "# Function to remove emoticons from text\n",
        "def remove_emoticons(text):\n",
        "    try:\n",
        "        EMOTICONS = {':)', ':-)', ':(', ':-(', ':D', ':-D', ':P', ':-P', 'XD', 'xD'} #well known emojis not all, manually inputed\n",
        "        emoticon_pattern = re.compile(u'(' + u'|'.join(re.escape(e) for e in EMOTICONS) + u')')\n",
        "        return emoticon_pattern.sub(r'', text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error removing emoticons: {e}\")\n",
        "        return text\n",
        "\n",
        "def process_files_in_folder(folder_path):\n",
        "    try:\n",
        "        for filename in os.listdir(folder_path):\n",
        "            file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "            if os.path.isfile(file_path):\n",
        "                try:\n",
        "                    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                        content = file.read()\n",
        "\n",
        "                    processed_content = remove_emoji(content)\n",
        "                    processed_content = remove_emoticons(processed_content)\n",
        "\n",
        "                    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "                        file.write(processed_content)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {file_path}: {e}\")\n",
        "            else:\n",
        "                print(f\"Skipping non-file entry: {file_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error iterating through folder {folder_path}: {e}\")\n",
        "\n",
        "folder_path = folder_path\n",
        "process_files_in_folder(folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13188db8-6bf6-41d4-8a43-b7e141dbcd72",
      "metadata": {
        "id": "13188db8-6bf6-41d4-8a43-b7e141dbcd72"
      },
      "source": [
        "for removing HTML tags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "025f18ce-7244-4daf-bae7-7fbd59002126",
      "metadata": {
        "id": "025f18ce-7244-4daf-bae7-7fbd59002126"
      },
      "outputs": [],
      "source": [
        "# Function to remove HTML tags using BeautifulSoup\n",
        "def remove_html_tags_bs4(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    return soup.get_text()\n",
        "\n",
        "# Function to remove HTML tags using regex\n",
        "def remove_html_tags_regex(text):\n",
        "    html_pattern = re.compile(r'<.*?>')\n",
        "    return html_pattern.sub(r'', text)\n",
        "\n",
        "# Function to remove HTML tags from files in the given folder\n",
        "def RemoveHTMLTags(folder, method='bs4'):\n",
        "    i = 0\n",
        "    try:\n",
        "        for root, dirs, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                old_file_path = os.path.join(root, file)\n",
        "                try:\n",
        "                    # Read the file content\n",
        "                    with open(old_file_path, 'r', encoding='utf-8') as f:\n",
        "                        content = f.read()\n",
        "\n",
        "                    # Remove HTML tags\n",
        "                    if method == 'bs4':\n",
        "                        content = remove_html_tags_bs4(content)\n",
        "                    elif method == 'regex':\n",
        "                        content = remove_html_tags_regex(content)\n",
        "                    else:\n",
        "                        raise ValueError(\"Invalid method for removing HTML tags. Choose 'bs4' or 'regex'.\")\n",
        "\n",
        "                    # Write the modified content back to the file\n",
        "                    with open(old_file_path, 'w', encoding='utf-8') as f:\n",
        "                        f.write(content)\n",
        "\n",
        "                except Exception as file_error:\n",
        "                    i += 1\n",
        "                    print(f\"Error processing file {old_file_path}: {file_error}\\n\")\n",
        "                except OSError as err:\n",
        "                    print(\"OS error:\", err)\n",
        "                except ValueError:\n",
        "                    print(\"Could not convert data to an integer.\")\n",
        "                except Exception as err:\n",
        "                    print(f\"Unexpected {err=}, {type(err)=}\")\n",
        "                    raise\n",
        "\n",
        "        print(\"Number of files that could not be processed:\", i)\n",
        "\n",
        "    except Exception as main_error:\n",
        "        print(f\"An error occurred while accessing the folder: {main_error}\")\n",
        "\n",
        "folder = folder_path\n",
        "RemoveHTMLTags(folder, method='bs4')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0e7c5b6-6481-4e25-bbe4-9f531143f543",
      "metadata": {
        "id": "f0e7c5b6-6481-4e25-bbe4-9f531143f543"
      },
      "source": [
        "Text chunks. Combine texts in smaller text files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "388c551e-9e6e-4d19-8305-33aade353d1e",
      "metadata": {
        "id": "388c551e-9e6e-4d19-8305-33aade353d1e"
      },
      "outputs": [],
      "source": [
        "#This function combines each sentence in a list with its neighboring sentences based on a specified buffer size\n",
        "#and appends the combined result as a new entry in each sentence's dictionary.\n",
        "def combine_sentences(sentences, buffer_size=1):\n",
        "    for i in range(len(sentences)):\n",
        "        combined_sentence = ''\n",
        "        for j in range(i - buffer_size, i):\n",
        "            if j >= 0:\n",
        "                combined_sentence += sentences[j]['sentence'] + ' '\n",
        "        combined_sentence += sentences[i]['sentence']\n",
        "\n",
        "        for j in range(i + 1, i + 1 + buffer_size):\n",
        "            if j < len(sentences):\n",
        "                combined_sentence += ' ' + sentences[j]['sentence']\n",
        "\n",
        "        sentences[i]['combined_sentence'] = combined_sentence\n",
        "    return sentences\n",
        "\n",
        "#This function calculates the cosine distance between the embeddings of consecutive combined sentences in a list\n",
        "#appends each distance to the list and updates each sentence's dictionary with the distance to the next sentence.\n",
        "def calculate_cosine_distance(sentences):\n",
        "    distances = []\n",
        "    for i in range(len(sentences)-1):\n",
        "        embedding_current = sentences[i].get('combined_sentence_embedding', None)\n",
        "        embedding_next = sentences[i+1].get('combined_sentence_embedding', None)\n",
        "\n",
        "        if embedding_current is not None and embedding_next is not None:\n",
        "            similarity = cosine_similarity([embedding_current], [embedding_next])[0][0]\n",
        "            distance = 1 - similarity\n",
        "            distances.append(distance)\n",
        "            sentences[i]['distance_to_next'] = distance\n",
        "        else:\n",
        "            distances.append(0.0)\n",
        "\n",
        "    return distances, sentences\n",
        "\n",
        "#This function processes a text file by splitting it into sentences, combining adjacent sentences into chunks based on cosine similarity of their embeddings\n",
        "#and saving chunks exceeding 2 kB into separate text files in a specified output folder.\n",
        "def split_into_chunks(input_file_path, output_folder):\n",
        "    try:\n",
        "        with open(input_file_path, 'r', encoding='utf-8') as f:\n",
        "            content = f.read()\n",
        "\n",
        "        single_sentences_list = re.split(r'(?<=[.?!])\\s+', content)\n",
        "        sentences = [{\"sentence\": x, \"index\" : i} for i, x in enumerate(single_sentences_list)]\n",
        "\n",
        "        sentences = combine_sentences(sentences)\n",
        "\n",
        "        model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "        combined_texts = [sentence['combined_sentence'] for sentence in sentences]\n",
        "        embeddings = model.encode(combined_texts)\n",
        "\n",
        "        for i, sentence in enumerate(sentences):\n",
        "            sentence['combined_sentence_embedding'] = embeddings[i]\n",
        "\n",
        "        distances, sentences = calculate_cosine_distance(sentences)\n",
        "\n",
        "        plt.plot(distances)\n",
        "        y_upper_bound = 0.65\n",
        "        plt.ylim(0, y_upper_bound)\n",
        "        plt.xlim(0, len(distances))\n",
        "        breakpoint = 95\n",
        "        breakpoint2 = np.percentile(distances, breakpoint)\n",
        "        plt.axhline(y=breakpoint2, color='r', linestyle='-')\n",
        "        #plt.show()\n",
        "\n",
        "        if not os.path.exists(output_folder):\n",
        "            os.makedirs(output_folder)\n",
        "\n",
        "        start = 0\n",
        "        chunk_counter = 0\n",
        "        for i, dist in enumerate(distances):\n",
        "            if dist > breakpoint2:\n",
        "                chunk_sentences = sentences[start:i+2]  # Include current and next sentence\n",
        "                combined_txt = ' '.join([sent['sentence'] for sent in chunk_sentences])\n",
        "\n",
        "                # Check if size > 2 kB\n",
        "                if len(combined_txt.encode('utf-8')) > 2000:\n",
        "                    chunk_counter += 1\n",
        "                    with open(os.path.join(output_folder, f'{os.path.basename(input_file_path)}_chunk_{chunk_counter}.txt'), 'w') as file:\n",
        "                        file.write(combined_txt)\n",
        "\n",
        "                start = i + 1\n",
        "\n",
        "        # Handle any remaining sentences\n",
        "        if start < len(sentences):\n",
        "            combined_txt = \" \".join(d[\"sentence\"] for d in sentences[start:])\n",
        "\n",
        "            # Check if size > 2 kB\n",
        "            if len(combined_txt.encode('utf-8')) > 2000:\n",
        "                chunk_counter += 1\n",
        "                with open(os.path.join(output_folder, f'{os.path.basename(input_file_path)}_chunk_{chunk_counter}.txt'), 'w') as file:\n",
        "                    file.write(combined_txt)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while processing {input_file_path}: {e}\")\n",
        "\n",
        "def process_folder(input_folder, output_folder):\n",
        "    for filename in os.listdir(input_folder):\n",
        "        input_file_path = os.path.join(input_folder, filename)\n",
        "        if os.path.isfile(input_file_path) and input_file_path.endswith('.txt'):\n",
        "            print(f\"Processing {input_file_path}...\")\n",
        "            split_into_chunks(input_file_path, output_folder)\n",
        "\n",
        "\n",
        "input_folder = folder_path\n",
        "output_folder = \"../input/textfiles\"\n",
        "process_folder(input_folder, output_folder)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}