{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "135ae549-2ecb-489d-82bb-a126a0c15103",
      "metadata": {
        "id": "135ae549-2ecb-489d-82bb-a126a0c15103"
      },
      "source": [
        "Prints 10 most used words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f20024d-be2e-4c73-a0d2-2701a755d153",
      "metadata": {
        "id": "6f20024d-be2e-4c73-a0d2-2701a755d153"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "stop_words_dutch = set(stopwords.words('dutch'))\n",
        "stop_words_dutch_list = list(stop_words_dutch)\n",
        "\n",
        "def get_top_tfidf_words(tfidf_scores, feature_names, top_n=10):\n",
        "    sorted_indices = tfidf_scores.argsort()[::-1][:top_n]\n",
        "    top_words = [(feature_names[i], tfidf_scores[i]) for i in sorted_indices]\n",
        "    return top_words\n",
        "\n",
        "def plot_top_tfidf_words(top_words):\n",
        "    words, scores = zip(*top_words)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.bar(words, scores, color='blue')\n",
        "    plt.xlabel('Words')\n",
        "    plt.ylabel('TF-IDF Score')\n",
        "    plt.title('Top Words in Text Files by TF-IDF Score')\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.savefig('Top_words_TF-IDF.png')\n",
        "    plt.show()\n",
        "\n",
        "def create_wordcloud(tfidf_scores, feature_names):\n",
        "    word_scores = {feature_names[i]: tfidf_scores[i] for i in range(len(feature_names))}\n",
        "\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(word_scores)\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.imshow(wordcloud, interpolation='bilinear')\n",
        "    plt.axis('off')\n",
        "    plt.title('Word Cloud of Top Words by TF-IDF Score')\n",
        "    plt.savefig('Word_Cloud.png')\n",
        "    plt.show()\n",
        "\n",
        "def analyze_text_files_with_tfidf(folders, top_n=10, manual_stopwords=None):\n",
        "    documents = []\n",
        "\n",
        "    if manual_stopwords is None:\n",
        "        combined_stopwords = stop_words_dutch_list\n",
        "    else:\n",
        "        combined_stopwords = stop_words_dutch_list + manual_stopwords\n",
        "\n",
        "    for folder in folders:\n",
        "        for filename in os.listdir(folder):\n",
        "            if filename.endswith('.txt'):\n",
        "                input_path = os.path.join(folder, filename)\n",
        "                try:\n",
        "                    with open(input_path, 'r', encoding='utf-8') as file:\n",
        "                        content = file.read()\n",
        "                    documents.append(content)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing file {filename}: {e}\")\n",
        "\n",
        "    vectorizer = TfidfVectorizer(stop_words=combined_stopwords)\n",
        "    tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "    avg_tfidf_scores = tfidf_matrix.mean(axis=0).A1\n",
        "    feature_names = vectorizer.get_feature_names_out()\n",
        "\n",
        "    top_words = get_top_tfidf_words(avg_tfidf_scores, feature_names, top_n)\n",
        "\n",
        "    plot_top_tfidf_words(top_words)\n",
        "    create_wordcloud(avg_tfidf_scores, feature_names)\n",
        "\n",
        "\n",
        "manual_stopwords_list = ['deze', 'die', 'het', 'we', 'wij', 'onze', 'wel', 'jouw']\n",
        "folders = [\n",
        "    '../input/data_folder/Part1',\n",
        "    '../input/data_folder/part2',\n",
        "    '../input/data_folder/part3',\n",
        "    '../input/data_folder/part4',\n",
        "    '../input/data_folder/part5',\n",
        "    '../input/data_folder/part6'\n",
        "]\n",
        "analyze_text_files_with_tfidf(folders, top_n=10, manual_stopwords=manual_stopwords_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "49e7e332-05ef-4001-9e8b-c298b0506f00",
      "metadata": {
        "id": "49e7e332-05ef-4001-9e8b-c298b0506f00"
      },
      "outputs": [],
      "source": [
        "file distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "91263840-894b-45f1-a795-b024bcfff7d6",
      "metadata": {
        "tags": [],
        "id": "91263840-894b-45f1-a795-b024bcfff7d6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def get_file_sizes(folders):\n",
        "    file_sizes = []\n",
        "\n",
        "    for folder in folders:\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if file.endswith('.txt'):\n",
        "                    try:\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        file_size = os.path.getsize(file_path)\n",
        "                        file_sizes.append((folder, file, file_size))\n",
        "                    except OSError as e:\n",
        "                        i = 1\n",
        "                        #print(f\"Error accessing file {file_path}: {e}\")\n",
        "\n",
        "    return file_sizes\n",
        "\n",
        "def print_size_distribution(file_sizes):\n",
        "    df = pd.DataFrame(file_sizes, columns=['folder', 'file', 'size'])\n",
        "\n",
        "    folder_mapping = {folder: f\"Folder {i+1}\" for i, folder in enumerate(df['folder'].unique())}\n",
        "    df['folder'] = df['folder'].map(folder_mapping)\n",
        "\n",
        "    # Print the distribution of file sizes for each folder\n",
        "    for folder in df['folder'].unique():\n",
        "        folder_sizes = df[df['folder'] == folder]['size']\n",
        "        print(f\"Folder: {folder}\")\n",
        "        print(f\"  Count: {folder_sizes.count()}\")\n",
        "        print(f\"  Mean: {folder_sizes.mean()}\")\n",
        "        print(f\"  Median: {folder_sizes.median()}\")\n",
        "        print(f\"  Std: {folder_sizes.std()}\")\n",
        "        print(f\"  Min: {folder_sizes.min()}\")\n",
        "        print(f\"  Max: {folder_sizes.max()}\")\n",
        "        print()\n",
        "\n",
        "def main():\n",
        "\n",
        "    folders = [\n",
        "        '../input/data_folder/Part1',\n",
        "        '../input/data_folder/part2',\n",
        "        '../input/data_folder/part3',\n",
        "        '../input/data_folder/part4',\n",
        "        '../input/data_folder/part5',\n",
        "        '../input/data_folder/part6'\n",
        "    ]\n",
        "\n",
        "    file_sizes = get_file_sizes(folders)\n",
        "\n",
        "    if not file_sizes:\n",
        "        print(\"No text files found or could not access files.\")\n",
        "        return\n",
        "\n",
        "    # Print the distribution of file sizes\n",
        "    print_size_distribution(file_sizes)\n",
        "\n",
        "    # Create a DataFrame\n",
        "    df = pd.DataFrame(file_sizes, columns=['folder', 'file', 'size'])\n",
        "\n",
        "    # Map folder paths to folder names\n",
        "    folder_mapping = {folder: f\"Folder {i+1}\" for i, folder in enumerate(folders)}\n",
        "    df['folder'] = df['folder'].map(folder_mapping)\n",
        "\n",
        "\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    sns.boxplot(data=df, y='folder', x='size', orient='h')\n",
        "\n",
        "    # Customize the plot\n",
        "    plt.ylabel(\"Folder\")\n",
        "    plt.xlabel(\"File Size (mega bytes)\")\n",
        "    plt.title('Distribution of Text File Sizes in Folders')\n",
        "    plt.savefig('File_distribution.png')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c8e0715-96b0-4417-a467-39bce072dcf0",
      "metadata": {
        "id": "8c8e0715-96b0-4417-a467-39bce072dcf0"
      },
      "source": [
        "Language detection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac044f3a-38ae-4dd5-b86f-c0bbdea2448a",
      "metadata": {
        "id": "ac044f3a-38ae-4dd5-b86f-c0bbdea2448a"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langdetect import detect, DetectorFactory, LangDetectException\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "DetectorFactory.seed = 0\n",
        "\n",
        "def detect_language(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "            return detect(text)\n",
        "    except LangDetectException:\n",
        "        return 'unknown'\n",
        "    except Exception as e:\n",
        "        #print(f\"Error reading file {file_path}: {e}\")\n",
        "        return 'error'\n",
        "\n",
        "def get_languages(folders):\n",
        "    languages = []\n",
        "\n",
        "    for folder in folders:\n",
        "        for root, _, files in os.walk(folder):\n",
        "            for file in files:\n",
        "                if file.endswith('.txt'):\n",
        "                    file_path = os.path.join(root, file)\n",
        "                    language = detect_language(file_path)\n",
        "                    languages.append((file, language))\n",
        "\n",
        "    return languages\n",
        "\n",
        "def plot_language_distribution(languages, excluded_languages=None):\n",
        "    df = pd.DataFrame(languages, columns=['file', 'language'])\n",
        "\n",
        "    if excluded_languages:\n",
        "        df = df[~df['language'].isin(excluded_languages)]\n",
        "\n",
        "    sns.set(style=\"whitegrid\")\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    sns.countplot(data=df, y='language', order=df['language'].value_counts().index, palette=\"viridis\")\n",
        "    plt.xlabel(\"Number of Files\")\n",
        "    plt.ylabel(\"Language\")\n",
        "    plt.title(\"Distribution of Languages in Text Files\")\n",
        "    plt.show()\n",
        "\n",
        "def main():\n",
        "\n",
        "    folders = [\n",
        "        '../input/data_folder/Part1',\n",
        "        '../input/data_folder/part2',\n",
        "        '../input/data_folder/part3',\n",
        "        '../input/data_folder/part4',\n",
        "        '../input/data_folder/part5',\n",
        "        '../input/data_folder/part6'\n",
        "    ]\n",
        "\n",
        "    languages = get_languages(folders)\n",
        "    excluded_languages = ['ro','et','pl','es','hi','pt','sk','it','ca','sq','cy']  # Add languages you want to exclude here\n",
        "\n",
        "    plot_language_distribution(languages, excluded_languages)\n",
        "    plt.savefig('../img/Language_distribution.png')\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}